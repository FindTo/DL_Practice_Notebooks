{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# In this series of tasks, you will need to create embeddings for objects from a subset of the IMDB dataset using pre-trained models from Hugging Face.\n\nIn this task, create them using the BERT model (bert-base-cased) / RoBERTa (roberta-base) / DistilBERT (distilbert-base-cased) and use the get_embeddings_labels function from the seminar.\n\n```python\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"imdb\", split=\"train\")\n\nnp.random.seed(100)\nidx = np.random.randint(len(dataset), size=200)\n```\n\nPlease verify before submission that the tensor with embeddings has the dimensions (200, 768).","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy as np\n\nfrom warnings import filterwarnings\n\nfilterwarnings('ignore')\n\nfrom transformers import AutoTokenizer\nfrom transformers import BertModel  # https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertModel\nfrom transformers import RobertaModel  # https://huggingface.co/docs/transformers/model_doc/roberta#transformers.RobertaModel\nfrom transformers import DistilBertModel  # https://huggingface.co/docs/transformers/model_doc/distilbert#transformers.DistilBertModel\n\n\ndef get_model(model_name):\n    assert model_name in ['bert', 'roberta', 'distilbert']\n    \n    checkpoint_names = {\n        'bert': 'bert-base-cased',  # https://huggingface.co/bert-base-cased\n        'roberta': 'roberta-base',  # https://huggingface.co/roberta-base\n        'distilbert': 'distilbert-base-cased'  # https://huggingface.co/distilbert-base-cased\n    }\n    \n    model_classes = {\n        'bert': BertModel,\n        'roberta': RobertaModel,\n        'distilbert': DistilBertModel\n    }\n    \n    return AutoTokenizer.from_pretrained(checkpoint_names[model_name]), model_classes[model_name].from_pretrained(checkpoint_names[model_name])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T16:20:10.752288Z","iopub.execute_input":"2025-10-24T16:20:10.752594Z","iopub.status.idle":"2025-10-24T16:20:27.614667Z","shell.execute_reply.started":"2025-10-24T16:20:10.752558Z","shell.execute_reply":"2025-10-24T16:20:27.613659Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"#tokenizer, model = get_model('bert')\ntokenizer, model = get_model('distilbert')\n#tokenizer, model = get_model('roberta')\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\nprint(device)\n\nmodel = model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T16:20:27.616512Z","iopub.execute_input":"2025-10-24T16:20:27.617412Z","iopub.status.idle":"2025-10-24T16:20:30.620943Z","shell.execute_reply.started":"2025-10-24T16:20:27.617371Z","shell.execute_reply":"2025-10-24T16:20:30.620170Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc87b3de86d84b0caeb82e1d3a542530"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/465 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f20e289859e84ecba66ad9814c23ee57"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ced318d6daa641ea8408b3bf81789da7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec1a5dd33264468284dae262a83c754e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/263M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b573f12d41a47f49b2ce455639c59fa"}},"metadata":{}},{"name":"stdout","text":"cuda:0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset(\"imdb\", split=\"train\")\n\nnp.random.seed(100)\nidx = np.random.randint(len(dataset), size=200)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T16:20:30.622074Z","iopub.execute_input":"2025-10-24T16:20:30.622429Z","iopub.status.idle":"2025-10-24T16:20:36.084147Z","shell.execute_reply.started":"2025-10-24T16:20:30.622390Z","shell.execute_reply":"2025-10-24T16:20:36.083092Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1056e13ca94c4ce1a1028f6f68f36108"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f93b444ab93848aca8b59c8de3159d4c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c4394cd8b1c447e87d4e1983cb3d37b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"unsupervised-00000-of-00001.parquet:   0%|          | 0.00/42.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9ee553684584c5c9155ed2b51a89614"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f96b40f2bf6a46ca9652cd38cd2f63d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f841920eebe44db9d78d60b8d7b76ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f3395f706f0485aa6366fd35a1f2bf1"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"idx","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T16:20:36.085856Z","iopub.execute_input":"2025-10-24T16:20:36.086852Z","iopub.status.idle":"2025-10-24T16:20:36.096423Z","shell.execute_reply.started":"2025-10-24T16:20:36.086795Z","shell.execute_reply":"2025-10-24T16:20:36.095133Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"array([ 5640, 23320, 14147, 24423, 12119,    79, 16304, 16734, 14260,\n       10082, 20533, 13890,  7906,  5646, 17186, 13297, 10992, 20760,\n       11492, 17724,  7738, 11152,  9723,  5769, 15453,  6230, 18818,\n        5787, 18284, 21377,  3213,  6901, 22227, 12900, 10843, 12219,\n       19267, 20615, 11695, 10557, 13070, 20151, 21243, 19792, 16386,\n         889,  4713,  2526, 16354, 19637, 12571, 23790, 20382,  2992,\n       11567,  3949,   806, 10412,  8466, 18624, 23480,  7074, 20917,\n        1738, 20753,  9741, 15390,  8209,  7221,  6212,   488, 19803,\n       15182, 15360,  4109,  5305, 10602,  4227, 19014,  1667, 16079,\n       24970, 13527, 20975, 19305, 13488, 11444, 14123,  1797, 17223,\n        6182,  3286,  5290, 11092,  6239, 13938, 24301, 20087,  7969,\n       14650, 14438,  7424, 11002,  9911,  8418,  4914, 17860,  4861,\n       15280,  3568, 18197, 24985, 19894,  8454,  1701, 22374,  5748,\n       11559, 20075, 18142,  5939, 21369,  2626, 20723,  4535, 22801,\n        4909,  9701,  2137,   429, 17187,  1040,  8022, 11730, 14957,\n       24556, 11487,  8980,  6282, 12761, 23240,  5534,  1932, 19445,\n        1373, 23415, 13916,   556, 19701,  6527,  9960, 17078,  3036,\n       16016,  9605, 21939, 16730, 10274, 17246, 14286,  2742,  5197,\n        4308, 19745,   148,  3608,  7645, 18874, 13084, 22482, 23250,\n       11911, 19934, 10815, 23409, 23682, 13527, 11994,  9466, 21216,\n        1822,  3139,  8860, 12197, 21586,  5094, 10122, 17037, 10833,\n       19225, 23746, 17039,  4460,  6266, 10251,  6757, 14374, 14196,\n       14102, 23351])"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom transformers import DataCollatorWithPadding\nfrom torch.utils.data import Subset\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\ndef tokenization(example):\n    return tokenizer.batch_encode_plus(example['text'], add_special_tokens=True, return_token_type_ids=False, truncation=True)\n\n\ntoken_dataset = dataset.map(tokenization, batched=True)\n\ntoken_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n\nloader = DataLoader(Subset(token_dataset, idx), batch_size=64, collate_fn=data_collator, pin_memory=True, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T16:20:36.098369Z","iopub.execute_input":"2025-10-24T16:20:36.098678Z","iopub.status.idle":"2025-10-24T16:20:47.719623Z","shell.execute_reply.started":"2025-10-24T16:20:36.098650Z","shell.execute_reply":"2025-10-24T16:20:47.718480Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc9f6c8922be47608320c22406e75b22"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"from tqdm import tqdm\n\n@torch.inference_mode()\ndef get_embeddings_labels(model, loader):\n    model.eval()\n    \n    total_embeddings = []\n    labels = []\n    \n    for batch in tqdm(loader):\n        labels.append(batch['labels'].unsqueeze(1))\n\n        batch = {key: batch[key].to(device) for key in ['attention_mask', 'input_ids']}\n\n        embeddings = model(**batch)['last_hidden_state'][:, 0, :]\n\n        total_embeddings.append(embeddings.cpu())\n\n    return torch.cat(total_embeddings, dim=0), torch.cat(labels, dim=0).to(torch.float32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T16:20:47.721102Z","iopub.execute_input":"2025-10-24T16:20:47.721426Z","iopub.status.idle":"2025-10-24T16:20:47.729460Z","shell.execute_reply.started":"2025-10-24T16:20:47.721394Z","shell.execute_reply":"2025-10-24T16:20:47.728328Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"embeddings, labels = get_embeddings_labels(model, loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T16:20:47.730913Z","iopub.execute_input":"2025-10-24T16:20:47.731319Z","iopub.status.idle":"2025-10-24T16:20:50.964394Z","shell.execute_reply.started":"2025-10-24T16:20:47.731275Z","shell.execute_reply":"2025-10-24T16:20:50.963551Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 4/4 [00:03<00:00,  1.32it/s]\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"torch.save(embeddings, 'predictions_distilbert.pt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T16:20:50.965592Z","iopub.execute_input":"2025-10-24T16:20:50.965848Z","iopub.status.idle":"2025-10-24T16:20:50.971728Z","shell.execute_reply.started":"2025-10-24T16:20:50.965824Z","shell.execute_reply":"2025-10-24T16:20:50.970899Z"}},"outputs":[],"execution_count":8}]}